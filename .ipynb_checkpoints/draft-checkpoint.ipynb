{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.statement = df.statement.apply(lambda x: txtTrans(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "folder = glob.glob('./data_sessions/*/*.txt')\n",
    "files  = [i for i in folder if int(i[-8:-4]) >= 2010 and int(i[-8:-4]) <= 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of documents:', len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: George Chen, UDA HW3\n",
    "\n",
    "import re\n",
    "import string\n",
    "import codecs\n",
    "\n",
    "def makeWordList(path):\n",
    "    \n",
    "    with codecs.open(path, \"r\", encoding = 'utf-8', errors = 'ignore') as f:\n",
    "        corpus_text = f.read()\n",
    "\n",
    "    for c in string.punctuation:\n",
    "        corpus_text = corpus_text.replace(c, \"\")  \n",
    "    \n",
    "    text = re.sub(r'\\S*\\d\\S*','',corpus_text) \n",
    "    text = re.sub(r'[^\\w\\s]','',text)         \n",
    "    \n",
    "    text = text.lower().split()           \n",
    "    \n",
    "    li = []\n",
    "    for token in text:\n",
    "        li.append(token)\n",
    "\n",
    "    return \" \".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs    = [makeWordList(i) for i in files]\n",
    "year    = [int(i.split('\\\\')[2][7:-4]) for i in files]\n",
    "country = [i.split('\\\\')[2][:3] for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(year):\n",
    "    if j == 11: year[i] = 2011\n",
    "    if j == 12: year[i] = 2012\n",
    "    if j == 13: year[i] = 2013\n",
    "    if j == 14: year[i] = 2014\n",
    "    if j == 15: year[i] = 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "temp = np.unique(year, return_counts = True)\n",
    "for i in range(6): print(temp[0][i], ':', temp[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.unique(country, return_counts = True)\n",
    "print('Countries with less participation:', *(temp[0][i] for i in range(196) if temp[1][i] < 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tokens by doc\n",
    "# Remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stopW  = set(stopwords.words('english'))\n",
    "tokens = []\n",
    "for i in docs: \n",
    "    words = word_tokenize(i)\n",
    "    words = [j for j in words if j not in stopW]\n",
    "    tokens.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "temp = list(itertools.chain(*tokens[195:]))   # analysis for 2015\n",
    "\n",
    "plt.plot()\n",
    "cloud = WordCloud(width = 900, height = 400).generate(' '.join(temp)).to_image()\n",
    "plt.imshow(cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.earthdatascience.org/courses/earth-analytics-python/using-apis-natural-language-processing-twitter/calculate-tweet-word-bigrams-networks-in-python/\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "\n",
    "bigrams = [list(bigrams(i)) for i in tokens[195:]]\n",
    "flatten = list(itertools.chain(*bigrams))\n",
    "count   = Counter(flatten)\n",
    "pd.DataFrame(count.most_common(20), columns = ['bigram', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "temp = pd.DataFrame(count.most_common(20), columns = ['bigram', 'count']).set_index('bigram').T.to_dict('records')\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for k, v in temp[0].items():\n",
    "    G.add_edge(k[0], k[1], weight=(v * 10))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "pos = nx.spring_layout(G, k=1)\n",
    "\n",
    "# Plot networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=16,\n",
    "                 width=3,\n",
    "                 edge_color='grey',\n",
    "                 node_color='purple',\n",
    "                 with_labels = False,\n",
    "                 ax=ax)\n",
    "\n",
    "# Create offset labels\n",
    "for key, value in pos.items():\n",
    "    x, y = value[0]+.235, value[1]+.145\n",
    "    ax.text(x, y,\n",
    "            s=key,\n",
    "            bbox=dict(facecolor='red', alpha=0.25),\n",
    "            horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
